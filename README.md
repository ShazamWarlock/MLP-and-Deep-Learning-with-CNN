# MLP-and-Deep-Learning-with-CNN
In this task, we aim to implement a 3 layer MLP from scratch without using any in-built
libraries. We have chosen sigmoid function as our activation function and used Mean Square
Error (MSE) as an estimate for loss. Batch backpropagation has been used in the learning stage
with a learning rate of 2. In this experiment we will be discussing the reasons for choosing all the
parameters, along with the flow of the code and analysis of the result.
